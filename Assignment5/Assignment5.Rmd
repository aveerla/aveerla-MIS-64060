---
title: "Assignment5"
author: "Anujeeth Veerla"
date: "28/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
library(cluster)
library(tidyverse)
library(factoextra)
library(dendextend)
library(knitr)
```
```{r}
Cereals_df<-read.csv("Cereals.csv")
#Removing all cereal missing values from the CSV
sum(is.na(Cereals_df))
C<-na.omit(Cereals_df) #C has 74 obs where as initial file has 77. 
sum(is.na(C))
#removing categorical variables before scaling
C1<-C[,c(-1,-2,-3)] 
head(C1)
#Scaling the data
Cs<-scale(C1)
```
Apply hierarchical clustering to the data using Euclidean distance to the normalized 
measurements. Use Agnes to compare the clustering from  single linkage, complete 
linkage, average linkage, and Ward. Choose the best method. 
```{r}
#Calculating dissimilarity matrix
dis<-dist(Cs, method = "euclidean")
#Performing hierarchical clustering using complete linkage
hc_complete<-hclust(dis, method = "complete")
plot(hc_complete, cex=0.6, hang=-1)
#Performing clustering using Agnes 
c.single<-agnes(Cs, method = "single")
c.complete<-agnes(Cs, method = "complete")
c.average<-agnes(Cs,method = "average")
c.ward<-agnes(Cs, method = "ward")
#Now, we can compare agglomerative coefficients of single,complete,average and ward methods.
print(c.single$ac)
print(c.complete$ac)
print(c.average$ac)
print(c.ward$ac)
#Plotting the dendogram using wards method
pl<-pltree(c.ward,cex=0.5,hang=-1,main = "Dendogram of agnes-ward method")
```

Q) How many clusters would you choose? 
#Distance matrix
```{r}

distn<-dist(Cs, method = "euclidean")
```
#Wards method for hierarchial cluster
```{r}
w_hc<-hclust(distn,method = "ward.D2")
```
plotting the dendogram and value of k is taken as 4 by observing the tree
```{r}
plot(w_hc,cex=0.5)
rect.hclust(w_hc,k=4,border=1:4)
```
Identifying clusters
```{r}
clust<-cutree(w_hc,k=4)
```
total no. of members in each cluster
```{r}
table(clust)
```


Q) Comment on the structure of the clusters and on their stability. Hint: To check stability,  
partition the data and see how well clusters formed based on one part apply to the other 
part. 
```{r}
set.seed(123)
Cereals_new<-Cereals_df
```
removing missing values from the csv dataset
```{r}
n<-na.omit(Cereals_new)
n1<-n[,c(-1,-2,-3)]
n2<-scale(n1)
n3<-as.data.frame(n2)
```
Creating partitions
```{r}
par1<-n[1:55,]
par2<-n[56:74,]
```
Clustering using agnes with partitioned data
```{r}
r1<-agnes(scale(par1[,-c(1:3)]),method = "ward")
r2<-agnes(scale(par1[,-c(1:3)]),method = "average")
r3<-agnes(scale(par1[,-c(1:3)]),method = "complete")
r4<-agnes(scale(par1[,-c(1:3)]),method = "single")
cbind(ward=r1$ac,average=r2$ac,complete=r3$ac,single=r4$ac)
c2<-cutree(r1,k=4)
```
Calculating the centers
```{r}
ce<-as.data.frame(cbind(scale(par1[,-c(1:3)]),c2))
ce1<-colMeans(ce[ce$c2==1,])
ce2<-colMeans(ce[ce$c2==2,])
ce3<-colMeans(ce[ce$c2==3,])
ce4<-colMeans(ce[ce$c2==4,])
```
Binding 4 centers
```{r}
centers<-rbind(ce1, ce2, ce3, ce4)
centers
```
#Calculating distance    
```{r}
a<-as.data.frame(rbind(centers[,-14],scale(par2[,-c(1:3)])))
a1<-get_dist(a)
a2<-as.matrix(a1)
d1<-data.frame(data=seq(1,nrow(par2),1),clusters=rep(0,nrow(par2)))
for (i in 1:nrow(par2)) 
  {
  d1[i,2]<-which.min(a2[i+4,1:4])
}
d1
a3<-as.data.frame(cbind(Cs,clust))
cbind(a3$clust[56:74],d1$clusters)
table(a3$clust[56:74]==d1$clusters)
```
As we have more no of trues, model is stable

Q)find a cluster of “healthy cereals.” 
```{r}
Cr<-cbind(n3,clust)
Cr[Cr$clust==1,]
Cr[Cr$clust==2,]
Cr[Cr$clust==3,]
Cr[Cr$clust==4,]
```
Determining the best cluster using mean values
```{r}
mean(Cr[Cr$clust==1,"rating"])
mean(Cr[Cr$clust==2,"rating"])
mean(Cr[Cr$clust==3,"rating"])
mean(Cr[Cr$clust==4,"rating"])
```
Cluster 1 has the highest mean rating. So, we can choose cluster 1. 



